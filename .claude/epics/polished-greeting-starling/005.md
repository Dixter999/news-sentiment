---
name: Implement CLI Orchestrator
status: open
created: 2025-11-30T10:22:44Z
updated: 2025-11-30T10:22:44Z
github: https://github.com/Dixter999/news-sentiment/issues/7
depends_on: [003, 004]
parallel: false
conflicts_with: []
---

# Task: Implement CLI Orchestrator

## Description
Implement the main CLI entry point (`main.py`) that orchestrates the scrape → analyze → store workflow. Support command-line arguments for scraping period, analysis, and test-run mode.

## TDD Requirements
**This project uses Test-Driven Development. You MUST:**
1. RED: Write failing test first (test CLI arguments and workflow)
2. GREEN: Write minimal code to make test pass
3. REFACTOR: Clean up code while keeping tests green

See `.claude/rules/tdd.enforcement.md` for complete requirements.

## Acceptance Criteria
- [ ] CLI parses arguments: --scrape, --analyze, --test-run
- [ ] --scrape accepts: today, week, month
- [ ] --analyze processes unscored events
- [ ] --test-run prevents database commits
- [ ] Workflow: Scrape → Store → Analyze (if requested)
- [ ] Progress output shows event counts
- [ ] Error handling for API/DB failures
- [ ] Can combine --scrape and --analyze

## Technical Details

### File: `src/main.py`

```python
import argparse
from datetime import datetime
from scraper.ff_scraper import ForexFactoryScraper
from analyzer.gemini import SentimentAnalyzer
from database.models import EconomicEvent
from database.connection import get_session

def main():
    parser = argparse.ArgumentParser(
        description="News Sentiment Service - Scrape and analyze economic events"
    )
    parser.add_argument(
        "--scrape",
        choices=["today", "week", "month"],
        help="Scrape period"
    )
    parser.add_argument(
        "--analyze",
        action="store_true",
        help="Analyze unscored events with Gemini"
    )
    parser.add_argument(
        "--test-run",
        action="store_true",
        help="Test run (no database commits)"
    )
    args = parser.parse_args()

    if not args.scrape and not args.analyze:
        parser.print_help()
        return

    scraper = ForexFactoryScraper()
    analyzer = SentimentAnalyzer()

    if args.scrape:
        events = scrape_events(scraper, args.scrape)
        print(f"Scraped {len(events)} events")

        if not args.test_run:
            store_events(events)
            print("Events stored in database")

    if args.analyze:
        count = analyze_events(analyzer, args.test_run)
        print(f"Analyzed {count} events")


def scrape_events(scraper: ForexFactoryScraper, period: str) -> list:
    """Scrape events for the specified period."""
    now = datetime.now()
    if period == "today":
        return scraper.scrape_day(now)
    elif period == "week":
        return scraper.scrape_week(now)
    elif period == "month":
        return scraper.scrape_month(now.year, now.month)


def store_events(events: list) -> int:
    """Store scraped events in database."""
    with get_session() as session:
        for event in events:
            db_event = EconomicEvent(**event)
            session.merge(db_event)  # Upsert
        session.commit()
    return len(events)


def analyze_events(analyzer: SentimentAnalyzer, test_run: bool) -> int:
    """Analyze unscored events with Gemini."""
    with get_session() as session:
        unscored = session.query(EconomicEvent).filter(
            EconomicEvent.sentiment_score.is_(None),
            EconomicEvent.actual.isnot(None)  # Only analyze events with results
        ).all()

        for event in unscored:
            result = analyzer.analyze(event.to_dict())
            event.sentiment_score = result["sentiment_score"]
            event.raw_response = result["raw_response"]
            print(f"  {event.event_name}: {result['sentiment_score']:.2f}")

        if not test_run:
            session.commit()

    return len(unscored)


if __name__ == "__main__":
    main()
```

### CLI Usage Examples
```bash
# Scrape this week's events
python src/main.py --scrape week

# Analyze unscored events
python src/main.py --analyze

# Full pipeline
python src/main.py --scrape week --analyze

# Test run (no DB writes)
python src/main.py --scrape week --analyze --test-run
```

### Test File: `tests/test_main.py`
```python
class TestCLI:
    def test_scrape_week_calls_scraper(self, mock_scraper):
        """--scrape week calls scraper.scrape_week()."""

    def test_analyze_processes_unscored(self, mock_analyzer, db_with_unscored):
        """--analyze processes events without sentiment_score."""

    def test_test_run_no_commit(self, mock_scraper, mock_analyzer):
        """--test-run prevents database commits."""

    def test_combined_scrape_and_analyze(self):
        """Can combine --scrape and --analyze."""

    def test_no_args_shows_help(self, capsys):
        """No arguments shows help text."""
```

## Dependencies
- [ ] Task 003: Scraper implemented
- [ ] Task 004: Analyzer implemented

## Effort Estimate
- Size: S
- Hours: 4
- Parallel: false

## Definition of Done
- [ ] Tests written FIRST (RED phase)
- [ ] Code implemented (GREEN phase)
- [ ] Code refactored (REFACTOR phase)
- [ ] All tests passing
- [ ] CLI arguments work correctly
- [ ] Full workflow executes
- [ ] Test-run mode works
