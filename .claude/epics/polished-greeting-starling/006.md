---
name: Write Integration Tests
status: open
created: 2025-11-30T10:22:44Z
updated: 2025-11-30T10:22:44Z
github: https://github.com/Dixter999/news-sentiment/issues/8
depends_on: [005]
parallel: true
conflicts_with: []
---

# Task: Write Integration Tests

## Description
Write comprehensive integration tests for the full pipeline: Scrape → Analyze → Store. Include tests for database operations, error recovery, and end-to-end workflow validation.

## TDD Requirements
**This project uses Test-Driven Development. You MUST:**
1. RED: Write failing test first
2. GREEN: Write minimal code to make test pass
3. REFACTOR: Clean up code while keeping tests green

See `.claude/rules/tdd.enforcement.md` for complete requirements.

## Acceptance Criteria
- [ ] Integration test for scraper → database storage
- [ ] Integration test for database → analyzer → update
- [ ] End-to-end test for full pipeline
- [ ] Test for duplicate event handling (upsert)
- [ ] Test for partial failure recovery
- [ ] Test coverage > 80%
- [ ] All tests run in CI/CD

## Technical Details

### Test File: `tests/test_integration.py`

```python
import pytest
from datetime import datetime, timedelta
from src.scraper.ff_scraper import ForexFactoryScraper
from src.analyzer.gemini import SentimentAnalyzer
from src.database.models import EconomicEvent
from src.database.connection import get_session

class TestScraperToDatabase:
    """Integration tests for scraper → database flow."""

    def test_scraped_events_stored_correctly(self, test_db):
        """Scraped events are stored with all fields."""
        scraper = ForexFactoryScraper(headless=True)

        # Scrape a known date with events
        events = scraper.scrape_week(datetime(2024, 1, 1))

        with get_session() as session:
            for event in events:
                db_event = EconomicEvent(**event)
                session.merge(db_event)
            session.commit()

        # Verify storage
        with get_session() as session:
            stored = session.query(EconomicEvent).count()
            assert stored == len(events)

    def test_duplicate_events_upserted(self, test_db):
        """Duplicate events update existing records."""
        event = {
            "timestamp": datetime(2024, 1, 5, 13, 30),
            "currency": "USD",
            "event_name": "Non-Farm Payrolls",
            "impact": "High",
            "actual": "216K",
            "forecast": "175K",
            "previous": "173K",
        }

        # Insert once
        with get_session() as session:
            session.merge(EconomicEvent(**event))
            session.commit()

        # Update actual value
        event["actual"] = "220K"
        with get_session() as session:
            session.merge(EconomicEvent(**event))
            session.commit()

        # Should still be one record
        with get_session() as session:
            count = session.query(EconomicEvent).filter_by(
                event_name="Non-Farm Payrolls"
            ).count()
            assert count == 1

            # Actual should be updated
            stored = session.query(EconomicEvent).filter_by(
                event_name="Non-Farm Payrolls"
            ).first()
            assert stored.actual == "220K"


class TestAnalyzerToDatabase:
    """Integration tests for analyzer → database flow."""

    def test_sentiment_score_stored(self, test_db, mock_gemini):
        """Sentiment score is stored after analysis."""
        # Insert unscored event
        with get_session() as session:
            event = EconomicEvent(
                timestamp=datetime(2024, 1, 5, 13, 30),
                currency="USD",
                event_name="Non-Farm Payrolls",
                impact="High",
                actual="216K",
                forecast="175K",
                previous="173K",
            )
            session.add(event)
            session.commit()

        # Analyze
        analyzer = SentimentAnalyzer()
        with get_session() as session:
            unscored = session.query(EconomicEvent).filter(
                EconomicEvent.sentiment_score.is_(None)
            ).first()

            result = analyzer.analyze(unscored.to_dict())
            unscored.sentiment_score = result["sentiment_score"]
            unscored.raw_response = result["raw_response"]
            session.commit()

        # Verify
        with get_session() as session:
            stored = session.query(EconomicEvent).first()
            assert stored.sentiment_score is not None
            assert stored.raw_response is not None


class TestEndToEnd:
    """End-to-end integration tests."""

    def test_full_pipeline(self, test_db, mock_gemini):
        """Full pipeline: scrape → store → analyze → update."""
        from src.main import scrape_events, store_events, analyze_events

        scraper = ForexFactoryScraper(headless=True)
        analyzer = SentimentAnalyzer()

        # Step 1: Scrape
        events = scrape_events(scraper, "week")
        assert len(events) > 0

        # Step 2: Store
        store_events(events)

        # Step 3: Analyze
        count = analyze_events(analyzer, test_run=False)
        assert count > 0

        # Step 4: Verify
        with get_session() as session:
            with_score = session.query(EconomicEvent).filter(
                EconomicEvent.sentiment_score.isnot(None)
            ).count()
            assert with_score > 0

    def test_partial_failure_recovery(self, test_db, mock_gemini_partial_fail):
        """Pipeline continues after individual event analysis fails."""
        # Create multiple events
        events = [
            {"timestamp": datetime(2024, 1, 5, 8, 30), ...},
            {"timestamp": datetime(2024, 1, 5, 10, 0), ...},  # This will fail
            {"timestamp": datetime(2024, 1, 5, 14, 0), ...},
        ]

        # Store events
        with get_session() as session:
            for event in events:
                session.add(EconomicEvent(**event))
            session.commit()

        # Analyze (one fails)
        analyzer = SentimentAnalyzer()
        analyze_events(analyzer, test_run=False)

        # 2 of 3 should have scores
        with get_session() as session:
            with_score = session.query(EconomicEvent).filter(
                EconomicEvent.sentiment_score.isnot(None)
            ).count()
            assert with_score == 2
```

### Fixtures: `tests/conftest.py`
```python
import pytest
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from src.database.models import Base

@pytest.fixture(scope="function")
def test_db():
    """Create test database for each test."""
    engine = create_engine("sqlite:///:memory:")
    Base.metadata.create_all(engine)
    yield engine
    Base.metadata.drop_all(engine)

@pytest.fixture
def mock_gemini(mocker):
    """Mock Gemini API responses."""
    mock = mocker.patch("google.generativeai.GenerativeModel.generate_content")
    mock.return_value.text = '{"score": 0.75, "reasoning": "NFP beat forecast"}'
    return mock

@pytest.fixture
def mock_gemini_partial_fail(mocker):
    """Mock Gemini API with one failure."""
    responses = [
        '{"score": 0.5, "reasoning": "ok"}',
        Exception("API Error"),
        '{"score": -0.3, "reasoning": "ok"}',
    ]
    mock = mocker.patch("google.generativeai.GenerativeModel.generate_content")
    mock.side_effect = responses
    return mock
```

## Dependencies
- [ ] Task 005: CLI orchestrator implemented

## Effort Estimate
- Size: M
- Hours: 6
- Parallel: true

## Definition of Done
- [ ] Tests written FIRST (RED phase)
- [ ] Code implemented (GREEN phase)
- [ ] Code refactored (REFACTOR phase)
- [ ] All tests passing
- [ ] Coverage > 80%
- [ ] Integration tests documented
